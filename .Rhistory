)
nrounds <- 200
# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales
tune_grid1 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = c(0.05, 0.1, 0.3),
max_depth = c(2, 3, 4, 5),
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
xgb_tune1 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid1,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid2 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = xgb_tune1$bestTune$eta,
max_depth = xgb_tune1$bestTune$max_depth,
gamma = 0,
colsample_bytree = 1,
min_child_weight = c(1, 2, 3),
subsample = 1
)
xgb_tune2 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid2,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid3 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = xgb_tune1$bestTune$eta,
max_depth = xgb_tune1$bestTune$max_depth,
gamma = 0,
colsample_bytree = c(0.33, 0.66, 1.0),
min_child_weight = xgb_tune2$bestTune$min_child_weight,
subsample = c(0.5, 0.75, 1.0)
)
xgb_tune3 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid3,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid4 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = xgb_tune1$bestTune$eta,
max_depth = xgb_tune1$bestTune$max_depth,
gamma = c(0, 0.1, 0.5, 0.8, 1.0),
colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
min_child_weight = xgb_tune2$bestTune$min_child_weight,
subsample = xgb_tune3$bestTune$subsample
)
xgb_tune4 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid4,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid5 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = seq(xgb_tune1$bestTune$eta,xgb_tune1$bestTune$eta/20,length.out = 5),
max_depth = xgb_tune1$bestTune$max_depth,
gamma = xgb_tune4$bestTune$gamma,
colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
min_child_weight = xgb_tune2$bestTune$min_child_weight,
subsample = xgb_tune3$bestTune$subsample
)
xgb_tune5 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid5,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
model_list = list(v1=xgb_tune1,
v2=xgb_tune2,
v3=xgb_tune3,
v4=xgb_tune4,
v5=xgb_tune5,
xgboost_base=xgb_base
)
resamps <- resamples(model_list)
summary(resamps, metric = "RMSE")
dotplot(resamps, metric = "RMSE")
print(xgb_tune5$bestTune)
bestTune = xgb_tune5$bestTune
write.csv(bestTune,paste(model_dir,"bestXGB.csv",sep = "/"),row.names = F)
}
if(!file.exists(paste(model_dir,"xgb.model",sep="/"))) {
bestTune = read.csv(paste(model_dir,"bestXGB.csv",sep = "/"), header = TRUE, sep = ",")
grid_best <- expand.grid(
nrounds = bestTune$nrounds,
max_depth = bestTune$max_depth,
eta = bestTune$eta,
gamma = bestTune$gamma,
colsample_bytree = bestTune$colsample_bytree,
min_child_weight = bestTune$min_child_weight,
subsample = bestTune$subsample
)
xgb_best <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = grid_best,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
saveRDS(xgb_best, paste(model_dir,"xgb.model",sep="/"))
} else {
xgb_best = load(paste(model_dir,"xgb.model",sep="/"))
}
TUNE_XGB = F
if(TUNE_XGB) {
grid_default <- expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
xgb_base <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = grid_default,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
nrounds <- 200
# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales
tune_grid1 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = c(0.05, 0.1, 0.3),
max_depth = c(2, 3, 4, 5),
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
xgb_tune1 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid1,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid2 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = xgb_tune1$bestTune$eta,
max_depth = xgb_tune1$bestTune$max_depth,
gamma = 0,
colsample_bytree = 1,
min_child_weight = c(1, 2, 3),
subsample = 1
)
xgb_tune2 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid2,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid3 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = xgb_tune1$bestTune$eta,
max_depth = xgb_tune1$bestTune$max_depth,
gamma = 0,
colsample_bytree = c(0.33, 0.66, 1.0),
min_child_weight = xgb_tune2$bestTune$min_child_weight,
subsample = c(0.5, 0.75, 1.0)
)
xgb_tune3 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid3,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid4 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = xgb_tune1$bestTune$eta,
max_depth = xgb_tune1$bestTune$max_depth,
gamma = c(0, 0.1, 0.5, 0.8, 1.0),
colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
min_child_weight = xgb_tune2$bestTune$min_child_weight,
subsample = xgb_tune3$bestTune$subsample
)
xgb_tune4 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid4,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid5 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = seq(xgb_tune1$bestTune$eta,xgb_tune1$bestTune$eta/20,length.out = 5),
max_depth = xgb_tune1$bestTune$max_depth,
gamma = xgb_tune4$bestTune$gamma,
colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
min_child_weight = xgb_tune2$bestTune$min_child_weight,
subsample = xgb_tune3$bestTune$subsample
)
xgb_tune5 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid5,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
model_list = list(v1=xgb_tune1,
v2=xgb_tune2,
v3=xgb_tune3,
v4=xgb_tune4,
v5=xgb_tune5,
xgboost_base=xgb_base
)
resamps <- resamples(model_list)
summary(resamps, metric = "RMSE")
dotplot(resamps, metric = "RMSE")
print(xgb_tune5$bestTune)
bestTune = xgb_tune5$bestTune
write.csv(bestTune,paste(model_dir,"bestXGB.csv",sep = "/"),row.names = F)
}
if(!file.exists(paste(model_dir,"xgb.model",sep="/"))) {
bestTune = read.csv(paste(model_dir,"bestXGB.csv",sep = "/"), header = TRUE, sep = ",")
grid_best <- expand.grid(
nrounds = bestTune$nrounds,
max_depth = bestTune$max_depth,
eta = bestTune$eta,
gamma = bestTune$gamma,
colsample_bytree = bestTune$colsample_bytree,
min_child_weight = bestTune$min_child_weight,
subsample = bestTune$subsample
)
xgb_best <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = grid_best,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
saveRDS(xgb_best, paste(model_dir,"xgb.model",sep="/"))
} else {
xgb_best = source(paste(model_dir,"xgb.model",sep="/"))
}
TUNE_XGB = F
if(TUNE_XGB) {
grid_default <- expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
xgb_base <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = grid_default,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
nrounds <- 200
# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales
tune_grid1 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = c(0.05, 0.1, 0.3),
max_depth = c(2, 3, 4, 5),
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
xgb_tune1 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid1,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid2 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = xgb_tune1$bestTune$eta,
max_depth = xgb_tune1$bestTune$max_depth,
gamma = 0,
colsample_bytree = 1,
min_child_weight = c(1, 2, 3),
subsample = 1
)
xgb_tune2 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid2,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid3 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = xgb_tune1$bestTune$eta,
max_depth = xgb_tune1$bestTune$max_depth,
gamma = 0,
colsample_bytree = c(0.33, 0.66, 1.0),
min_child_weight = xgb_tune2$bestTune$min_child_weight,
subsample = c(0.5, 0.75, 1.0)
)
xgb_tune3 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid3,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid4 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = xgb_tune1$bestTune$eta,
max_depth = xgb_tune1$bestTune$max_depth,
gamma = c(0, 0.1, 0.5, 0.8, 1.0),
colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
min_child_weight = xgb_tune2$bestTune$min_child_weight,
subsample = xgb_tune3$bestTune$subsample
)
xgb_tune4 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid4,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
tune_grid5 <- expand.grid(
nrounds = seq(from = 50, to = nrounds, by = 50),
eta = seq(xgb_tune1$bestTune$eta,xgb_tune1$bestTune$eta/20,length.out = 5),
max_depth = xgb_tune1$bestTune$max_depth,
gamma = xgb_tune4$bestTune$gamma,
colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
min_child_weight = xgb_tune2$bestTune$min_child_weight,
subsample = xgb_tune3$bestTune$subsample
)
xgb_tune5 <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = tune_grid5,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
model_list = list(v1=xgb_tune1,
v2=xgb_tune2,
v3=xgb_tune3,
v4=xgb_tune4,
v5=xgb_tune5,
xgboost_base=xgb_base
)
resamps <- resamples(model_list)
summary(resamps, metric = "RMSE")
dotplot(resamps, metric = "RMSE")
print(xgb_tune5$bestTune)
bestTune = xgb_tune5$bestTune
write.csv(bestTune,paste(model_dir,"bestXGB.csv",sep = "/"),row.names = F)
}
if(!file.exists(paste(model_dir,"xgb.model",sep="/"))) {
bestTune = read.csv(paste(model_dir,"bestXGB.csv",sep = "/"), header = TRUE, sep = ",")
grid_best <- expand.grid(
nrounds = bestTune$nrounds,
max_depth = bestTune$max_depth,
eta = bestTune$eta,
gamma = bestTune$gamma,
colsample_bytree = bestTune$colsample_bytree,
min_child_weight = bestTune$min_child_weight,
subsample = bestTune$subsample
)
xgb_best <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = grid_best,
method = "xgbTree",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
saveRDS(xgb_best, paste(model_dir,"xgb.model",sep="/"))
} else {
xgb_best = readRDS(paste(model_dir,"xgb.model",sep="/"))
}
xgb_best
pred<-data.frame('Prediction'= predict(xgb_best,test_x),'True' = test_y)
p1 = ggplot(data=pred,aes(x=Prediction,y=True)) + geom_jitter() + geom_smooth(method='lm',size=.5) +
theme_minimal(12) + labs(title=paste0('Prediction vs. ground truth for ',xgb_best$method))
pred<-data.frame('Prediction'= predict(model_rf,test_x),'True' = test_y)
pred<-data.frame('Prediction'= predict(xgb_best,test_x),'True' = test_y)
ggplot(data=pred,aes(x=Prediction,y=True)) + geom_jitter() + geom_smooth(method='lm',size=.5) +
theme_minimal(12) + labs(title=paste0('Prediction vs. ground truth for ',xgb_best$method))
# pred<-data.frame('Prediction'= predict(model_rf,test_x),'True' = test_y)
# p2 = ggplot(data=pred,aes(x=Prediction,y=True)) + geom_jitter() + geom_smooth(method='lm',size=.5) +
#       theme_minimal(12) + labs(title=paste0('Prediction vs. ground truth for ',xgb_best$method))
# grid.arrange(g1,g2,ncol=1)
xgb_best
0:20
0:5
0:5/3
grid_svmLinear <- expand.grid(C=0:5/3)
model_svmLinear <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = grid_svmLinear,
method = "svmLinear",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
grid_svmLinear <- expand.grid(C=1)
model_svmLinear <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = grid_svmLinear,
method = "svmLinear",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
model_nnet <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = expand.grid(size=c(10), decay=c(0.1)),
method = "nnet",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
model_nnet <- caret::train(
x = train_x,
y = train_y,
metric = "RMSE",
trControl = train_control,
tuneGrid = expand.grid(size=c(20), decay=c(0.1)),
method = "nnet",
verbose = TRUE,
preProcess = c("zv", "nzv","center","scale")
)
