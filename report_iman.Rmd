---
title: "Beijing Housing Price Forecasting"
author: "TALL Machine Learning - Zheng (James) Lai, Iman Lau, Dung Tran"
date: "November 15, 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r warning=FALSE,message=FALSE}
library(plyr)
library(dplyr)
library(Hmisc)
library(ggpubr)
library(DMwR)
library(caret)
library(pROC)
library(ggplot2)
library(ggpubr)
library(stringr)
library(lubridate)
library(fastDummies)
library(Matrix)
library(onehot)
library(vtreat)
library(statsr)
library(knitr)
library(ggplot2)
library(dplyr)
library(RColorBrewer)
library(rworldmap)
library(ggthemes)
library(rgdal)
library(corrplot)
library(reshape2)
library(gridExtra)
library(caret)
library(psych)
library(reshape2)
library(doMC)
library(mice)
library(clustMixType)
library(rlist)

registerDoMC(6)
set.seed(42)

model_dir = "models"
data_dir = "data"

### Loading data
data=read.csv(paste(data_dir,"cleaned_data.csv",sep="/"), header = TRUE, sep = ",",na.strings = c("NA", "nan","#NAME?"))
```

## I. Introduction

The final project for the York Machine Learning course, *Machine Learning in Business Context* was to apply all that we have learned throughout the course towards one data set/project. We chose a dataset about the [Housing Prices in Beijing](https://www.kaggle.com/ruiqurm/lianjia). This report follows the structures laid out in CRISP-DM methodology.

The GitHub repository for all the source code is located at the following link: [https://github.com/stellarclass/Beijing-Housing](https://github.com/stellarclass/Beijing-Housing).

The RShiny app is located at the following link: [link here](link here).

## II. Business Understanding

Being able to predict the housing prices in the future has many uses. One can use this information to plan when and where to buy a house. It can also be used as to plan one's investments. Being able to buy a house when prices are cheaper and sell when prices are higher is one way to make money, after all.

## III. Data Understanding

The data set was provided courtesy of [Qichen Qiu](https://www.kaggle.com/ruiqurm) who scraped the Beijing housing site, [Lianjia.com](https://lianjia.com/).

The data contains various aspects of a house that prospective buyers would be interested in, some date-time attributes, and the location of the building.

The following table shows the house properties:

Attribute    | Description
-------------|------------------------------------------------------------------------
followers    | the number of people who follow the transaction (an indication of popularity)
totalPrice   | the total price of the house
price        | the average price per square metre
square       | the square metres in the house
livingRoom   | the number of bedrooms (translation error)
drawingRoom  | the number of living rooms (translation error)
kitchen      | the number of kitchens
bathroom     | the number of bathrooms
floor        | the floor on which the house is located, includes where this floor is located in the building (e.g. top, bottom, etc.)
buildingType | the type of building (e.g. tower, bungalow, etc.)
constructionTime | the year the building was constructed
renovationCondition | the condition of the exterior
buildingStructure | what the building is made of
ladderRatio  | how many ladders are present per resident
elevator     | if there is an elevator or not
subway       | if a subway station is nearby or not
communityAverage | the average price of a house in that community

The following table shows the date-time attributes:

Attribute    | Description
-------------|------------------------------------------------------------------------
tradeTime    | the date of the transaction
DOM          | the active days on market (indicates how quickly a house sold)
fiveYearsProperty | if the owner of the property has owned it for greater than or less than 5 years

The following table shows the location attributes:

Attribute    | Description
-------------|------------------------------------------------------------------------
Lng          | the longitude of the house
Lat          | the latitude of the house
Cid          | the community ID
district     | the district where the house is located

There are also two columns that identify the house listing on the website:

Attribute    | Description
-------------|------------------------------------------------------------------------
url          | the url of the house/transaction from where data was scraped
id           | the ID of the transaction

## IV. Data Exploration and Preparation

### (1) Visualizations for client-related attributes

### (2) Visualizations for the social/economic attributes

### (3) Data outliers

There appears to be some outliers in the ladder ration:

```{r}

boxplot(data$ladderRatio, las = 1, ylim=c(0,2))

```

It does not make sense to have more than one ladder per unit.

### (4) Data correlations

### (5) Possible inconsistencies in data

### (6) Data Preprocessing

The data needs a lot of cleaning done. First of all, there are some Chinese characters in the data which conveys meaning. We first replace these Chinese characters with their English meanings:
```{r}
### Replace Chinese charactor
data$floor = gsub("¨¦????", "high", data$floor)
data$floor = gsub("??????", "low", data$floor)
data$floor = gsub("?????", "mid", data$floor)
data$floor = gsub("??o??", "bottom", data$floor)
data$floor = gsub("¨¦????", "low", data$floor)
data$floor = gsub("????a????£¤", "unkown", data$floor)
data$floor = gsub("¨¦??¡é??¡¤¡¤????????????", "steel-concrete composite", data$floor)
data$floor = gsub("??¡¤¡¤??????????????????", "mixed", data$floor)

data$drawingRoom = gsub("?????", "mid", data$drawingRoom)
data$drawingRoom = gsub("¨¦????", "high", data$drawingRoom)
data$drawingRoom = gsub("??????", "low", data$drawingRoom)
data$drawingRoom = gsub("??o??", "bottom", data$drawingRoom)
data$drawingRoom = gsub("¨¦????", "top", data$drawingRoom)

# constructionTime has shift
data$constructionTime = gsub("????a????£¤", "unknown", data$constructionTime)

#Building type also have shift
data$bathRoom = gsub("????a????£¤", "unknown", data$bathRoom)
apply(data,2,function(x){sum(is.na(x))/length(x)*100})
```

We also need to split the floor data out to have height and type:

```{r}
#split floor to have height info 
temp <- as.data.frame(str_split_fixed(data$floor, " ", 2))
colnames(temp) <- c("floorType", "floorLevel")
data <- cbind(data, temp)
data$floorLevel=as.integer(data$floorLevel)
data <- data[, !"floor"]
```

Finally, there are 32 columns which have data that has been shifted, so they are removed.

```{r}

data <- data[-which(is.na(data$livingRoom)),]

```

Afterwards, we can adjust some other aspects of the data such as removing more unused columns and type conversion:

```{r}
#load cleaned data due to Chinese character encoding
data=read.csv(paste(data_dir,"cleaned_data.csv",sep="/"), header = TRUE, sep = ",",na.strings = c("NA", "nan","#NAME?"))

#type conversion
data$tradeTime = ymd(data$tradeTime)
data$drawingRoom = as.integer(data$drawingRoom)
data$bathRoom = as.integer(data$bathRoom)
data$district=as.factor(data$district)
data$constructionTime = as.integer(data$constructionTime)

data$buildingType = factor(mapvalues(data$buildingType, from = c(1, 2, 3, 4), to = c("tower", "bungalow", "plate-tower combination", "plate")))
data$renovationCondition = factor(mapvalues(data$renovationCondition, from = c(1, 2, 3, 4), to = c("other", "rough", "simplicity", "hardcover")))
data$buildingStructure = factor(mapvalues(data$buildingStructure, from = c(1, 2, 3, 4, 5, 6), to = c("unknown", "mixed", "brick and wood", "brick and concrete", "steel", "steel-concrete composite")))
data$elevator = factor(mapvalues(data$elevator, from = c(0, 1), to = c("no", "yes")))
data$fiveYearsProperty = factor(mapvalues(data$fiveYearsProperty, from = c(0, 1), to = c("no", "yes")))
data$subway = factor(mapvalues(data$subway, from = c(0, 1), to = c("no", "yes")))
```

As we noticed earlier, ladder ratio appears to have outliers, so we will now convert it to NA. The renovation condition also cannot be 0, so we change that to NA as well.

```{r}
data$ladderRatio[data$ladderRatio > 1] <- NA
data$renovationCondition[data$renovationCondition == 0] <- NA
```

Now we have some NA values to deal with.

```{r}
colSums(is.na(data))
```

We will use mice imputation to impute these missing values. We run the imputation on the data, excluding some less useful information in order to have it run faster. We also use the quick prediction option to help speed things up.

```{r eval = FALSE, cache = TRUE}
mice_mod <- mice(data[, !names(data) %in% c('Lng', 'Lat', 'Cid', 'followers', 'totalPrice', 'price')], pred = quickpred(data[, !names(data) %in% c('Lng', 'Lat', 'Cid', 'followers', 'totalPrice', 'price')], mincor=0.1, minpuc=0.3)) 

# Save the complete output 
mice_output <- complete(mice_mod)
```

We can now plot the distributions to see how the imputation looks and to make sure nothing looks out of place:

```{r}
#read in data because of length of time to impute
mice_output <- read.csv(paste(data_dir,"mice_output.csv",sep="/"), header = TRUE, sep = ",")

# Plot distributions
par(mfrow=c(1,2))
hist(data$DOM, freq=F, main='Original Data', 
     col='darkgreen', ylim=c(0,0.04))
hist(mice_output$DOM, freq=F, main='MICE Output', 
     col='lightgreen', ylim=c(0,0.04))

hist(as.numeric(data$buildingType), freq=F, main='Original Data', 
     col='darkgreen', ylim=c(0,0.04))
hist(as.numeric(mice_output$buildingType), freq=F, main='MICE Output', 
     col='lightgreen', ylim=c(0,0.04))

hist(data$ladderRatio, freq=F, main='Original Data', 
     col='darkgreen', ylim=c(0,0.04))
hist(mice_output$ladderRatio, freq=F, main='MICE Output', 
     col='lightgreen', ylim=c(0,0.04))

#doesn't work, don't run
# hist(as.numeric(data$communityAverage), freq=F, main='Original Data', 
#      col='darkgreen', ylim=c(0,0.04))
# hist(as.numeric(mice_output$communityAverage), freq=F, main='MICE Output', 
#      col='lightgreen', ylim=c(0,0.04))

hist(data$constructionTime, freq=F, main='Original Data', 
     col='darkgreen', ylim=c(0,0.04))
hist(mice_output$constructionTime, freq=F, main='MICE Output', 
     col='lightgreen', ylim=c(0,0.04))
```

Everything looks fine, so we can now assign the imputed values to the data set:

```{r}
data$DOM <- mice_output$DOM
data$buildingType <- mice_output$buildingType
data$ladderRatio <- mice_output$ladderRatio
data$communityAverage <- mice_output$communityAverage
data$constructionTime <- mice_output$constructionTime
```

## V. Modeling

```{r echo=FALSE}
#loading in mice imputed data due to the length of time it takes to impute
data=read.csv(paste(data_dir,"imputed_data.csv",sep="/"), header = TRUE, sep = ",")

# Convert to useful type
data$tradeTime = ymd(data$tradeTime)
data$year = year(data$tradeTime)
data = data[, !colnames(data) %in% c("tradeTime")]

data$drawingRoom = as.integer(data$drawingRoom)
data$bathRoom = as.integer(data$bathRoom)
data$buildingType = as.factor(data$buildingType)
data$constructionTime = as.integer(data$constructionTime)
data$renovationCondition = as.factor(data$renovationCondition)
data$buildingStructure = as.factor(data$buildingStructure)
data$elevator = as.factor(data$elevator)
data$fiveYearsProperty = as.factor(data$fiveYearsProperty)
data$subway = as.factor(data$subway)
data$district=as.factor(data$district)

# non informative columns; or factors that have too many levels like Cid
data = data[,!colnames(data) %in% c("url","id","Cid")]

for(col in names(data)) {
  if(!is.factor(data[,col])) {
    data[,col] = as.numeric(data[,col])
    next
  }
  if(col=="price") next
  f = as.formula(paste("price~",col,"-1",sep=""))
  m = model.matrix(f,data)
  data = data[,!colnames(data) %in% c(col)]
  data = cbind(data,m)
}
```

### (1) Clustering

We use clustering to explore the data a bit further. First, we create a data frame which contains the attributes we are interested in, namely everything except location data. This is because we want to see if there's a relationship with the other attributes, not where the houses are located. Then we run kproto to produce clusters.

```{r eval = FALSE}
data_clust <- data[, !colnames(data) %in% c("Lng", "Lat", "Cid", "district")]

set.seed(1234)

kproto_clusters = list()
for (i in seq(2,8,1)) {
  kproto_clusters = list.append(kproto_clusters,kproto(data_clust,i,lambda = 1))
}
```

```{r warning = FALSE}
kproto_clusters <- list.load(paste(data_dir,"kproto_cluster.rds",sep="/"))
wss = c()
for(kc in kproto_clusters) {
  wss = c(wss,kc$tot.withinss)
}
plot(seq(2,8,1), wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
```

We can see that 3 or 4 clusters is a good number of clusters. 

```{r warning = FALSE}
kproto_selection = kproto_clusters[[3]]
```

Let's map the clusters to see what it looks like:

```{r}
data3 <- data
data3$cluster_id = as.factor(kproto_selection$cluster)
bj_map <- data.frame(data3$price, data3$Lat, data3$Lng, data3$cluster_id)
colnames(bj_map) <- c('price', 'lat', 'lon', 'cluster_id')
sbbox <- make_bbox(lon = data3$Lng, lat = data3$Lat, f = 0.05)
my_map <- get_map(location = sbbox, maptype = "roadmap", scale = 2, color="color", zoom = 10)
ggmap(my_map) +
  geom_point(data=bj_map, aes(x = lon, y = lat, color = cluster_id), 
             size = 0.5, alpha = 1) +
  xlab('Longitude') +
  ylab('Latitude') +
  ggtitle('Place Holder')
```

There is a clear pattern to these clusters! Let's take a closer look at the attributes in these clusters:

```{r}
kproto_results <- data3 %>%
  dplyr::select(-price,-Lng,-Lat,-Cid,-district) %>%
  group_by(cluster_id) %>%
  do(the_summary = summary(.))
print(kproto_results$the_summary)
```

Cluster 1 is not on market for very long (15 days average but 1 day median!) meaning it is in high demand. The price is around 350-400, but it tends to be pretty small - 60-70 m^2^, although they are not bungalows and they tend to be in the middle of a building. Typically they were built in the 90's and tend to have been owned for over 5 years. While they may or may not have an elevator, they are near a subway.

Cluster 2 is on the market for a while - 34 days average, 12 days median. This is likely because they are very expensive, at about 600-700! They are also quite cramped at 60-70 m^2^ but tend to be 2 bedroom units. The floor distribution is relatively even except for being on the bottom (not many are there). The buildings are generally plate type and built in the 90s. They are also near the subway, but may or may not have an elevator. They tend to be owned for over 5 years.

Cluster 3 is not on market for too long, but slightly longer than cluster 1 - 1 day median, 17 days mean. They are pretty cheap at around 300-350, and are slightly bigger than cluster 1, at 70-80 m^2^ and generally 2 bedrooms. They are not bungalows and tend to be slightly newer, being built in the 2000's. This means they more often have elevators and are near the subway. They also are usually owned for over 5 years. There is a more even distribution of floors, but tend not to be on ground floor.

Cluster 4 sells very quickly - 1 day median, 10 days mean! They are also the cheapest at about 200-250. They are very slightly bigger than cluster 3 - 80-90 m^2 but roughly same number of rooms, and are definitely not bungalows. They also were built in the 2000's but are slightly newer than cluster 3. They may or may not have elevators but they do tend to not be near a subway. Slightly more than half of the units were owned for over 5 years, and there is an even distribution of floors with slightly less on the bottom.

Based on locations in the map, it looks like cluster 4 is the farthest from the centre. This means it may be more suburban living, which may account for why it is the cheapest but the biggest. Cluster 3 also outside of city centre around outskirts of the downtown, while clusters 1 and 2 are the downtown properties. Cluster 2 takes the longest to sell and is the most expensive because it's in prime real estate but they are small. It is probably hard to justify buying cluster 2 type units.

```{r}
data_train <- data.frame(data %>% filter(year<2017))
data_test <- data.frame(data %>% filter(year>=2017))
train_x = as.matrix(data_train[,!colnames(data_train) %in% c("price","year")])
train_y = data_train[,colnames(data_train) %in% c("price")]
test_x = as.matrix(data_test[,!colnames(data_test) %in% c("price","year")])
test_y = data_test[,colnames(data_test) %in% c("price")]
folds <- createFolds(train_y, k = 5)
train_control <- trainControl(
  method = "cv",
  index = folds,
  verboseIter = F,
  allowParallel = TRUE # FALSE for reproducible results 
)
```

```{r}
TUNE_XGB = F
if(TUNE_XGB) {
grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_base <- caret::train(
  x = train_x,
  y = train_y,
  metric = "RMSE",
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

nrounds <- 200
# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales
tune_grid1 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = c(0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_tune1 <- caret::train(
  x = train_x,
  y = train_y,
  metric = "RMSE",
  trControl = train_control,
  tuneGrid = tune_grid1,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

tune_grid2 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune1$bestTune$eta,
  max_depth = xgb_tune1$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3),
  subsample = 1
)

xgb_tune2 <- caret::train(
  x = train_x,
  y = train_y,
  metric = "RMSE",
  trControl = train_control,
  tuneGrid = tune_grid2,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

tune_grid3 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune1$bestTune$eta,
  max_depth = xgb_tune1$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.33, 0.66, 1.0),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

xgb_tune3 <- caret::train(
  x = train_x,
  y = train_y,
  metric = "RMSE",
  trControl = train_control,
  tuneGrid = tune_grid3,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

tune_grid4 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune1$bestTune$eta,
  max_depth = xgb_tune1$bestTune$max_depth,
  gamma = c(0, 0.1, 0.5, 0.8, 1.0),
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune4 <- caret::train(
  x = train_x,
  y = train_y,
  metric = "RMSE",
  trControl = train_control,
  tuneGrid = tune_grid4,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

tune_grid5 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = seq(xgb_tune1$bestTune$eta,xgb_tune1$bestTune$eta/20,length.out = 5),
  max_depth = xgb_tune1$bestTune$max_depth,
  gamma = xgb_tune4$bestTune$gamma,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune5 <- caret::train(
  x = train_x,
  y = train_y,
  metric = "RMSE",
  trControl = train_control,
  tuneGrid = tune_grid5,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

model_list = list(v1=xgb_tune1,
                  v2=xgb_tune2,
                  v3=xgb_tune3,
                  v4=xgb_tune4,
                  v5=xgb_tune5,
                  xgboost_base=xgb_base
                  )
resamps <- resamples(model_list)
summary(resamps, metric = "RMSE")
dotplot(resamps, metric = "RMSE")
print(xgb_tune5$bestTune)
bestTune = xgb_tune5$bestTune
write.csv(bestTune,paste(model_dir,"bestXGB.csv",sep = "/"),row.names = F)
}
if(!file.exists(paste(model_dir,"xgb.model",sep="/"))) {
  bestTune = read.csv(paste(model_dir,"bestXGB.csv",sep = "/"), header = TRUE, sep = ",")
  grid_best <- expand.grid(
    nrounds = bestTune$nrounds,
    max_depth = bestTune$max_depth,
    eta = bestTune$eta,
    gamma = bestTune$gamma,
    colsample_bytree = bestTune$colsample_bytree,
    min_child_weight = bestTune$min_child_weight,
    subsample = bestTune$subsample
  )
  xgb_best <- caret::train(
    x = train_x,
    y = train_y,
    metric = "RMSE",
    trControl = train_control,
    tuneGrid = grid_best,
    method = "xgbTree",
    verbose = TRUE,
    preProcess = c("zv", "nzv","center","scale")
  )
  saveRDS(xgb_best, paste(model_dir,"xgb.model",sep="/"))
} else {
  xgb_best = readRDS(paste(model_dir,"xgb.model",sep="/"))
}
```

model_rf$results
  mtry  splitrule min.node.size     RMSE  Rsquared      MAE   RMSESD   RsquaredSD    MAESD
1   53 extratrees             1 3623.599 0.9634308 1943.905 42.66113 0.0008186957 33.56537

## VI. Evaluation
```{r}
pred<-data.frame('Prediction'= predict(xgb_best,test_x),'True' = test_y)
ggplot(data=pred,aes(x=Prediction,y=True)) + geom_jitter() + geom_smooth(method='lm',size=.5) +
      theme_minimal(12) + labs(title=paste0('Prediction vs. ground truth for ',xgb_best$method))
# pred<-data.frame('Prediction'= predict(model_rf,test_x),'True' = test_y)
# p2 = ggplot(data=pred,aes(x=Prediction,y=True)) + geom_jitter() + geom_smooth(method='lm',size=.5) +
#       theme_minimal(12) + labs(title=paste0('Prediction vs. ground truth for ',xgb_best$method))
# grid.arrange(g1,g2,ncol=1)
```

## VII. Deployment

## VIII. Conclusions
