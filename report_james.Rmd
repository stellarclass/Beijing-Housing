---
title: "Beijing Housing - Core Project"
author: "TALL Machine Learning - Zheng (James) Lai, Iman Lau, Dung Tran"
date: "November 15, 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r warning=FALSE,message=FALSE}
library(plyr)
library(dplyr)
library(Hmisc)
library(ggpubr)
library(DMwR)
library(caret)
library(pROC)
library(ggplot2)
library(ggpubr)
library(stringr)
library(lubridate)
library(fastDummies)
library(Matrix)
library(onehot)
library(vtreat)
library(statsr)
library(knitr)
library(ggplot2)
library(dplyr)
library(RColorBrewer)
library(rworldmap)
library(ggthemes)
library(rgdal)
library(corrplot)
library(reshape2)
library(gridExtra)
library(lubridate)
library(caret)
library(psych)
library(reshape2)
#library(doMC)
library(xgboost)
#registerDoMC(4)
set.seed(42)
model_dir = "models"
data_dir = "data"
```

## I. Introduction

## II. Business Understanding

## III. Data Understanding

Attribute    | Description
-------------|------------------------------------------------------------------------

 - **url**:  the url which fetches the data
 - **id**:  the id of transaction
 - **Lng**:  and **Lat**  coordinates, using the **BD09** protocol. 
 - **Cid**:  community id
 - **tradeTime**:  the time of transaction
 - **DOM**:  active days on market.Know more in https://en.wikipedia.org/wiki/Days_on_market
 - **followers**:  the number of people follow the transaction.
 - **totalPrice**:  the total price in multiple of 10,000
 - **price**:  the average price by square
 - **square**:  the square of house
 - **livingRoom**:   the number of living room
 - **drawingRoom**:  the number of drawing room
 - **kitchen**:  the number of kitchen
 - **bathroom** the number of bathroom
 - **floor**:  the height of the house. I will turn the Chinese characters to English in the next version.
 - **buildingType**:  including tower( 1 ) , bungalow( 2 )，combination of plate and tower( 3 ), plate( 4 ).
 - **constructionTime**:  the time of construction
 - **renovationCondition**: including other( 1 ), rough( 2 ),Simplicity( 3 ), hardcover( 4 )
 - **buildingStructure**: including unknow( 1 ), mixed( 2 ), brick and wood( 3 ), brick and concrete( 4 ),steel( 5 ) and steel-concrete composite ( 6 ).
 - **ladderRatio**:  the proportion between number of residents on the same floor and number of elevator of ladder. It describes how many ladders a resident have on average.
 - **elevator** have ( 1 ) or not have elevator( 0 )
 - **fiveYearsProperty**:  if the owner have the property for less than 5 years, 

## IV. Data Exploration and Preparation

### (1) Visualizations 

Prepare data from original dataset for visualization
```{r}
data_initial=read.csv(paste(data_dir,"new.csv",sep="/"), header = TRUE, sep = ",")
dataVisualize <- data_initial[, !colnames(data_initial) %in% c("id", "Cid")]
dataVisualize$floor <- as.factor(as.numeric(gsub("\\D", "", dataVisualize$floor)))
dataVisualize$drawingRoom <- as.factor(as.numeric(gsub("\\D", "", dataVisualize$drawingRoom)))
dataVisualize$constructionTime <- as.factor(as.numeric(gsub("\\D", "", dataVisualize$constructionTime)))
dataVisualize$bathRoom <- as.factor(as.numeric(gsub("\\D", "", dataVisualize$bathRoom)))
dataVisualize$livingRoom <- as.factor(as.numeric(gsub("\\D", "", dataVisualize$livingRoom)))

#construction time has 0 and 1 listed for year of construction - incorrect so change them to NA
levels(dataVisualize$constructionTime)[levels(dataVisualize$constructionTime)=="0"] <- NA
levels(dataVisualize$constructionTime)[levels(dataVisualize$constructionTime)=="1"] <- NA

#bathroom has what appears to be years - change to NA
levels(dataVisualize$bathRoom)[levels(dataVisualize$bathRoom)=="1990"] <- NA
levels(dataVisualize$bathRoom)[levels(dataVisualize$bathRoom)=="1994"] <- NA
levels(dataVisualize$bathRoom)[levels(dataVisualize$bathRoom)=="1996"] <- NA
levels(dataVisualize$bathRoom)[levels(dataVisualize$bathRoom)=="2000"] <- NA
levels(dataVisualize$bathRoom)[levels(dataVisualize$bathRoom)=="2003"] <- NA
levels(dataVisualize$bathRoom)[levels(dataVisualize$bathRoom)=="2004"] <- NA
levels(dataVisualize$bathRoom)[levels(dataVisualize$bathRoom)=="2005"] <- NA
levels(dataVisualize$bathRoom)[levels(dataVisualize$bathRoom)=="2006"] <- NA
levels(dataVisualize$bathRoom)[levels(dataVisualize$bathRoom)=="2011"] <- NA

#building type has decimal places when it should not - change to NA
levels(dataVisualize$buildingType)[levels(dataVisualize$buildingType)=="0.048"] <- NA
levels(dataVisualize$buildingType)[levels(dataVisualize$buildingType)=="0.125"] <- NA
levels(dataVisualize$buildingType)[levels(dataVisualize$buildingType)=="0.25"] <- NA
levels(dataVisualize$buildingType)[levels(dataVisualize$buildingType)=="0.333"] <- NA
levels(dataVisualize$buildingType)[levels(dataVisualize$buildingType)=="0.375"] <- NA
levels(dataVisualize$buildingType)[levels(dataVisualize$buildingType)=="0.429"] <- NA
levels(dataVisualize$buildingType)[levels(dataVisualize$buildingType)=="0.5"] <- NA
levels(dataVisualize$buildingType)[levels(dataVisualize$buildingType)=="0.667"] <- NA


#factor elevator, fiveyearsproperty, subway and change to yes/no
dataVisualize$elevator = factor(mapvalues(dataVisualize$elevator, from = c(0, 1), to = c("no", "yes")))
dataVisualize$fiveYearsProperty = factor(mapvalues(dataVisualize$fiveYearsProperty, from = c(0, 1), to = c("no", "yes")))
dataVisualize$subway = factor(mapvalues(dataVisualize$subway, from = c(0, 1), to = c("no", "yes")))

#factor building type, kitchen
dataVisualize$buildingType <- as.factor(dataVisualize$buildingType)
dataVisualize$kitchen <- as.factor(dataVisualize$kitchen)
dataVisualize$district <- as.factor(dataVisualize$district)
#converting numeric to categorical
#building type
makeBuildingType <- function(x){
  if(!is.na(x)){
    if(x==1){
      return('Tower')
    }
    else if (x==2){
      return('Bungalow')
    }
    else if (x==3){
      return('Mix_Plate_Tower')
    }
    else if (x==4){
      return('Plate')
    }
    else return('wrong_coded')
  }
  else{return('missing')}
}
dataVisualize$buildingType <- sapply(dataVisualize$buildingType, makeBuildingType)
dataVisualize <- data.frame(dataVisualize %>% filter(buildingType != 'wrong_coded' & buildingType !='missing'))

#building condition
makeRenovationCondition <- function(x){
  if(x==1){
    return('Other')
  }
  else if (x==2){
    return('Rough')
  }
  else if (x==3){
    return('Simplicity')
  }
  else if (x==4){
    return('Hardcover')
  }
}
dataVisualize$renovationCondition <- sapply(dataVisualize$renovationCondition, makeRenovationCondition)

# Building Structure
makeBuildingStructure <- function(x){
  if(x==1){
    return('Unknown')
  }
  else if (x==2){
    return('Mix')
  }
  else if (x==3){
    return('Brick_Wood')
  }
  else if (x==4){
    return('Brick_Concrete')
  }
  else if (x==5){
    return('Steel')
  }
  else if (x==6){
    return('Steel_Concrete')
  }
}
dataVisualize$buildingStructure <- sapply(dataVisualize$buildingStructure, makeBuildingStructure)

# Adding 2 attributes Month and Year for Selling Time
dataVisualize$tradeYear <- year(dmy(dataVisualize$tradeTime))
dataVisualize$tradeMonth <- month(dmy(dataVisualize$tradeTime))
df3 <- data.frame(dataVisualize %>% na.omit())
df3$buildingType <- as.factor(df3$buildingType)
df3$buildingStructure <- as.factor(df3$buildingStructure)
df3$elevator <- as.factor(df3$elevator)
df3$fiveYearsProperty <- as.factor(df3$fiveYearsProperty)
df3$subway <- as.factor(df3$subway)
df3$district <- as.factor(df3$district)
df3$renovationCondition <- as.factor(df3$renovationCondition)
df3$tradeTimeTs <- as.Date(df3$tradeTime, format = "%Y-%m-%d")
df3$monthlyTradeTS <- as.Date(paste0(df3$tradeYear,'-',df3$tradeMonth,'-01'))
```
We can observe the Beijing Housing Market on total trading volume, trading value and average trading total proce over years to have a general view on this market trend.
```{r}
year_data <- subset(dataVisualize,!(dataVisualize$tradeYear < 2011))
      all_group <- group_by(year_data, district, tradeYear)
      beijing_by_volume <- dplyr::summarise(all_group, n=n())
ggplot(aes(x = tradeYear , y = n, fill = district), data = beijing_by_volume) +
        geom_bar(stat = 'identity', position = 'stack', width = 0.5) +
        coord_flip() +
        xlab('Year') +
        ylab('Trading Volume') +
        theme_minimal() +
        theme(plot.title = element_text(size = 16),
              axis.title = element_text(size = 12, face = "bold"),
              axis.text.x = element_text(angle = 90, hjust = 1, vjust = .4))

year_data <- subset(dataVisualize,!(dataVisualize$tradeYear < 2011))
      all_group <- group_by(year_data, district, tradeYear)
      beijing_by_value <- dplyr::summarise(all_group, value=sum(totalPrice)/1000)
      ggplot(aes(x = tradeYear , y = value, fill = district), data = beijing_by_value) +
        geom_bar(stat = 'identity', position = 'stack', width = 0.5) +
        coord_flip() +
        xlab('Year') +
        ylab('Trading Value (1000 Yuan)') +
        theme_minimal() +
        theme(plot.title = element_text(size = 16),
              axis.title = element_text(size = 12, face = "bold"),
              axis.text.x = element_text(angle = 90, hjust = 1, vjust = .4))

all_group <- group_by(dataVisualize, tradeYear)
        beijing_by_average_price <- dplyr::summarise(all_group, average=mean(totalPrice))
        ggplot(aes(x=tradeYear, y=average), data =beijing_by_average_price) +
        geom_line(size=1.5) +
            ylab('Year') +
            xlab('Average Total Price (10k Yuan)') +
            theme_grey() +
            theme(plot.title = element_text(size = 16),
                  axis.title = element_text(size = 12, face = "bold")) 

```
Look deeper into the comparison between districts by year, we can visualize how the market different over districts through years on both volume, value and average square price. Please view our application to view more options on date range and let's take a look on the best year of the market - 2016 for an example here.

```{r}
compareData <- subset(dataVisualize, dataVisualize$tradeYear == 2016)
all_group <- group_by(compareData, district, tradeYear)
      beijing_by_volume <- dplyr::summarise(all_group, n=n())
      ggplot(aes(x = district , y = n, fill = n), data = beijing_by_volume) +
        geom_bar(stat = 'identity', width = 0.5) +
        xlab('District') +
        ylab('Trading Volume') +
        theme_minimal() +
        theme(plot.title = element_text(size = 16),
              axis.title = element_text(size = 12, face = "bold"),
              axis.text.x = element_text(angle = 90, hjust = 1, vjust = .4))

all_group <- group_by(compareData, district)
      beijing_by_value <- dplyr::summarise(all_group, value=sum(totalPrice)/1000)
      ggplot(aes(x = district , y = value, fill = value), data = beijing_by_value) +
        geom_bar(stat = 'identity', width = 0.5) +
        xlab('District') +
        ylab('Trading Value (1000 Yuan)') +
        theme_minimal() +
        theme(plot.title = element_text(size = 16),
              axis.title = element_text(size = 12, face = "bold"),
              axis.text.x = element_text(angle = 90, hjust = 1, vjust = .4))
      

mypalette <- colorRampPalette(brewer.pal(12,'Paired'))(13)
      mycols <- 3
      mybox <- compareData %>% ggplot(aes_string('district','price')) + geom_boxplot(aes_string(color='district')) + 
        scale_color_manual(name='',values=mypalette) + theme_minimal(12) + 
        theme(axis.title =element_blank(), legend.position='None') + 
        coord_flip()
      
grid.arrange(mybox, ncol=1)

```

We also can take a look on some typical categorical attributes to see how square price is depended on these features.
```{r}
makeFeatureCatEDA <- function(x){

    mypalette <-'Paired'
    mycols <- 2
    
    mybox <- df3 %>% ggplot(aes_string(x,'price')) + geom_boxplot(aes_string(color=x)) + 
      scale_color_brewer(name='', palette=mypalette) + theme_minimal(12) + 
      theme(axis.title =element_blank(), legend.position='None') + 
      labs(title='Average Price Per Square') #+ coord_flip()
  
    grid.arrange(mybox, ncol=1)
}
makeFeatureCatEDA('buildingStructure')
makeFeatureCatEDA('buildingType')
makeFeatureCatEDA('renovationCondition')
makeFeatureCatEDA('elevator')
makeFeatureCatEDA('subway')
makeFeatureCatEDA('fiveYearsProperty')
 
```
### (2) Data correlations
Our imputed dataset has been cleaned and transformed all attributes into numeric or binary values, so we will investigate the correlations between features on this dataset. For a better view, please visit our application.
```{r}
corrplot(cor(
      data %>% select_if(is.numeric) %>% select(-Lng, -Lat) ,use = "pairwise.complete.obs",
      method='pearson')
      ,method='ellipse',
      tl.cex = 1,
      col = viridis::viridis(50),
      tl.col='black')

```

### (3) Data outliers

### (4) Possible inconsistencies in data

### (5) Data Preprocessing
First of all, we want to convert all Chinese characters in the dataset into English.
```{r}
### Loading data
data=read.csv(paste(data_dir,"original_data.csv",sep="/"), header = TRUE, sep = ",",na.strings = c("nan","#NAME?"), fileEncoding="gbk")
data$floor = gsub("高", "high", data$floor)
data$floor = gsub("低", "low", data$floor)
data$floor = gsub("中", "mid", data$floor)
data$floor = gsub("底", "bottom", data$floor)
data$floor = gsub("顶", "low", data$floor)
data$floor = gsub("未知", "unkown", data$floor)
data$floor = gsub("钢混结构", "steel-concrete composite", data$floor)
data$floor = gsub("混合结构", "mixed", data$floor)
data$drawingRoom = gsub("中", "mid", data$drawingRoom)
data$drawingRoom = gsub("高", "high", data$drawingRoom)
data$drawingRoom = gsub("低", "low", data$drawingRoom)
data$drawingRoom = gsub("底", "bottom", data$drawingRoom)
data$drawingRoom = gsub("顶", "top", data$drawingRoom)
data$constructionTime = gsub("未知", NA, data$constructionTime)
data$bathRoom = gsub("未知", NA, data$bathRoom)
data$Cid = as.factor(data$Cid)
```
We also find that, through inspecting some of the original URL, several data seems to be misplaced into wrong columns such that off-the-chart values are present in these columns. For example, some rows have 2006 bathrooms. It seems more appropriate to say that the construction time for this row is 2006. Moreover, all these rows have missing values. We infer that these bad data points probably result from bugs in the crawler. There are only a handful of such data points. So we drop them directly.
```{r}
data = data[-which(is.na(data$livingRoom)),]
```

# Mice imputed data
```{r}
data=read.csv(paste(data_dir,"imputed_data.csv",sep="/"), header = TRUE, sep = ",")

# Convert to useful type
data$tradeTime = ymd(data$tradeTime)
data$year = year(data$tradeTime)
data = data[, !colnames(data) %in% c("tradeTime")]

data$drawingRoom = as.integer(data$drawingRoom)
data$bathRoom = as.integer(data$bathRoom)
data$buildingType = as.factor(data$buildingType)
data$constructionTime = as.integer(data$constructionTime)
data$renovationCondition = as.factor(data$renovationCondition)
data$buildingStructure = as.factor(data$buildingStructure)
data$elevator = as.factor(data$elevator)
data$fiveYearsProperty = as.factor(data$fiveYearsProperty)
data$subway = as.factor(data$subway)
data$district=as.factor(data$district)

# non informative columns; or factors that have too many levels like Cid
data = data[,!colnames(data) %in% c("url","id","Cid", "totalPrice", "X")]
```

## V. Modeling

For the predictive modeling of this dataset, we will attempt to build a model for "price", which is the total price over square meters of the property. We think this would be a good indicator for the general affordability in Beijing city as opposed to "totalPrice". We have divided the dataset into train and test set by the year: all data before 2017 will be the training set and all after 2017 will be test set. This mimics the situation where people use historical data to predict future housing price in Beijing. After all, there is no point in predicting a past transaction because we can simply look it up. Credit: [this report] (https://www.kaggle.com/jonathanbouchet/forecasting-beijing-s-housing).

We also prepared two types of data for training and testing. First is the data within which the factors are not explicitly dummy-coded. This is for our own convenience when writing a formula for the glm models.
```{r}
data_train_nodummy <- data.frame(data %>% filter(year<2017))
data_test_nodummy <- data.frame(data %>% filter(year>=2017))
```

The second type of data we need to prepare is the one that explicitly dummy-encodes the factor variables. We need to use this since we will be using advanced modeling package xgboost, which can only handle numeric data. And finally, we have defined 5-folds cross-validation as the evaluation methods for our modeling tasks.
```{r}
for(col in names(data)) {
  if(!is.factor(data[,col])) {
    data[,col] = as.numeric(data[,col])
    next
  }
  if(col=="price") next
  f = as.formula(paste("price~",col,"-1",sep=""))
  m = model.matrix(f,data)
  data = data[,!colnames(data) %in% c(col)]
  data = cbind(data,m)
}

data_train <- data.frame(data %>% filter(year<2017))
data_test <- data.frame(data %>% filter(year>=2017))
train_x = as.matrix(data_train[,!colnames(data_train) %in% c("price")])
train_y = data_train[,colnames(data_train) %in% c("price")]
test_x = as.matrix(data_test[,!colnames(data_test) %in% c("price")])
test_y = data_test[,colnames(data_test) %in% c("price")]
folds <- createFolds(train_y, k = 5)
train_control <- trainControl(
  method = "cv",
  index = folds,
  verboseIter = F,
  allowParallel = TRUE # FALSE for reproducible results 
)
```

### Generalized Linear Model(glm)
We have selected linear regression as our baseline model. A small trick that is applied here is manual feature selection according to the P values produced by the linear regression program. The step to manually select features are:
1. Run glm models on the dataset. 
2. Exclude the features that the glm models consider having high P value.
3. Rerun glmnet models using the resulting features after step 2.
Steps 1 and 2 are not shown here. Only the results for step 3 is shown. According to the P-value criteria, "living room" and "elevator" does not have significance over the price.
```{r}
model_glm <- caret::train(
    price ~.-livingRoom-elevator, data_train_nodummy,
    metric = "Rsquared",
    trControl = train_control,
    method = "glm",
    preProcess = c("zv", "nzv","center","scale")
)
model_glm$results
```
We see that the linear regression models are not particularly good. In terms of R-squared, it reaches 0.77. The glmnet model, which comes with regularization, is no better than plain glm model. And we won't bother to show the glmnet model results here. This suggests that we have an underfitting problem resulting from a model that is too simple. We also noticed that we have only experimented a 1st order linear model. But before we attempt to make a higher-order model, there is another problem, that is, which predictors do we choose to combine and make high order predictors? Let's make some guess for the formula:

```{r}
model_glm_highorder <- caret::train(
    price ~renovationCondition*elevator*communityAverage*subway*floorType, 
    data_train_nodummy,
    metric = "Rsquared",
    trControl = train_control,
    method = "glm",
    preProcess = c("zv", "nzv","center","scale")
)
model_glm_highorder$results
```

As it turns out, the 5th-order linear model becomes worse. If we go down this path, one big hyper-parameter we have to tune seems to be the number of order and the combination of predictors that make high order predictors. And it seems the only way to do this is an exhaustive search. We will look for two more complex models to see if there are smarter algorithms that can give us better results.

### Decision Tree Models
Previously we mentioned that a linear regression has an underfitting problem. We now attempt to fit the dataset using a decision tree model, with the "cp" as the hyperparameter tunning.
```{r}
model_rpart <- caret::train(
    x = train_x,
    y = train_y,
    metric = "Rsquared",
    tuneGrid = expand.grid(
      cp = c(0:5/50)
    ),
    trControl = train_control,
    method = "rpart",
    preProcess = c("zv", "nzv","center","scale")
)
plot(model_rpart)
```
Small complexity parameter results in larger trees and potential overfitting, large complexity parameter in small trees and potential underfitting. As we can see from the plot of RMSE vs complexity parameter, cp=0 still gets the lowest RMSE. This suggests that the model still suffers from underfitting. We continue to look for some other model that is more complex. 

### Extreme Gradiant Boost
Next, we investigate a powerful but complex model, xgboost. As we will see shortly, out-of-box xgboost on this dataset yield ~15% reduction in RMSE as compared to the decision tree model. With some tuning, we are able to yield 18% reduction in RMSE as compared to the rpart model. We have follows [this](https://www.kaggle.com/pelkoja/visual-xgboost-tuning-with-caret) tutorial in learning how to tune the xgboost model. 

The tuning process takes a long time to run. We have run these steps and save all intermediate models in the file systems. The following codes just document the tuning process.
```{r}
TUNE_XGB = F
if(TUNE_XGB) {
grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_base <- caret::train(
  x = train_x,
  y = train_y,
  metric = "Rsquared",
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

nrounds <- 500
# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales
tune_grid1 <- expand.grid(
  nrounds = seq(from = 100, to = nrounds, by = 100),
  eta = c(0.05, 0.1, 0.3),
  max_depth = c(2, 4, 6, 8),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_tune1 <- caret::train(
  x = train_x,
  y = train_y,
  metric = "Rsquared",
  trControl = train_control,
  tuneGrid = tune_grid1,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

tune_grid2 <- expand.grid(
  nrounds = seq(from = 100, to = nrounds, by = 100),
  eta = xgb_tune1$bestTune$eta,
  max_depth = xgb_tune1$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3),
  subsample = 1
)

xgb_tune2 <- caret::train(
  x = train_x,
  y = train_y,
  metric = "Rsquared",
  trControl = train_control,
  tuneGrid = tune_grid2,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

tune_grid3 <- expand.grid(
  nrounds = seq(from = 100, to = nrounds, by = 100),
  eta = xgb_tune1$bestTune$eta,
  max_depth = xgb_tune1$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.33, 0.66, 1.0),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

xgb_tune3 <- caret::train(
  x = train_x,
  y = train_y,
  metric = "Rsquared",
  trControl = train_control,
  tuneGrid = tune_grid3,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

tune_grid4 <- expand.grid(
  nrounds = seq(from = 100, to = nrounds, by = 100),
  eta = xgb_tune1$bestTune$eta,
  max_depth = xgb_tune1$bestTune$max_depth,
  gamma = c(0, 0.1, 0.5, 0.8, 1.0),
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune4 <- caret::train(
  x = train_x,
  y = train_y,
  metric = "Rsquared",
  trControl = train_control,
  tuneGrid = tune_grid4,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

tune_grid5 <- expand.grid(
  nrounds = seq(from = 100, to = nrounds, by = 100),
  eta = seq(xgb_tune1$bestTune$eta,xgb_tune1$bestTune$eta/20,length.out = 5),
  max_depth = xgb_tune1$bestTune$max_depth,
  gamma = xgb_tune4$bestTune$gamma,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune5 <- caret::train(
  x = train_x,
  y = train_y,
  metric = "Rsquared",
  trControl = train_control,
  tuneGrid = tune_grid5,
  method = "xgbTree",
  verbose = TRUE,
  preProcess = c("zv", "nzv","center","scale")
)

write.csv(bestTune,paste(model_dir,"bestXGB.csv",sep = "/"),row.names = F)
# Retrain the best models
grid_best <- expand.grid(
    nrounds = bestTune$nrounds,
    max_depth = bestTune$max_depth,
    eta = bestTune$eta,
    gamma = bestTune$gamma,
    colsample_bytree = bestTune$colsample_bytree,
    min_child_weight = bestTune$min_child_weight,
    subsample = bestTune$subsample
  )
xgb_best <- caret::train(
    x = train_x,
    y = train_y,
    metric = "Rsquared",
    trControl = train_control,
    tuneGrid = grid_best,
    method = "xgbTree",
    verbose = TRUE,
    preProcess = c("zv", "nzv","center","scale")
  )
saveRDS(xgb_best, paste(model_dir,"xgb.model",sep="/"))
saveRDS(xgb_base, paste(model_dir,"xgb_base.model",sep="/"))
saveRDS(xgb_tune1, paste(model_dir,"xgb_tune1.model",sep="/"))
saveRDS(xgb_tune2, paste(model_dir,"xgb_tune2.model",sep="/"))
saveRDS(xgb_tune3, paste(model_dir,"xgb_tune3.model",sep="/"))
saveRDS(xgb_tune4, paste(model_dir,"xgb_tune4.model",sep="/"))
saveRDS(xgb_tune5, paste(model_dir,"xgb_tune5.model",sep="/"))
}
xgb_best = readRDS(paste(model_dir,"xgb.model",sep="/"))
xgb_base = readRDS(paste(model_dir,"xgb_base.model",sep="/"))
xgb_tune1 = readRDS(paste(model_dir,"xgb_tune1.model",sep="/"))
xgb_tune2 = readRDS(paste(model_dir,"xgb_tune2.model",sep="/"))
xgb_tune3 = readRDS(paste(model_dir,"xgb_tune3.model",sep="/"))
xgb_tune4 = readRDS(paste(model_dir,"xgb_tune4.model",sep="/"))
xgb_tune5 = readRDS(paste(model_dir,"xgb_tune5.model",sep="/"))
model_list = list(v1=xgb_tune1,
                  v2=xgb_tune2,
                  v3=xgb_tune3,
                  v4=xgb_tune4,
                  v5=xgb_tune5,
                  xgboost_base=xgb_base
                  )
resamps <- resamples(model_list)
summary(resamps, metric = "Rsquared")
dotplot(resamps, metric = "Rsquared")
print(xgb_tune5$bestTune)
bestTune = xgb_tune5$bestTune
```
We have obtained the best hyperparameters for xgboost according to 5-folds cross-validation. A final comparison of RMSE of three models based on cross-validation is as follows.
```{r}
model_list = list(glm=model_glm,
                  rpart=model_rpart,
                  xgboost_base=xgb_base,
                  xgboost_best=xgb_best
                  )
resamps <- resamples(model_list)
summary(resamps, metric = "Rsquared")
dotplot(resamps, metric = "Rsquared")
```

## VI. Evaluation
We now apply the models we have previously trained on a "future dataset" - the transaction data from 2017 and beyond. Recall that we have trained the models using data before 2017 and try to predict housing price after 2017. In the following, the true price vs predicted price is plotted. The blue line is the ideal prediction, where all predicted price match 100% with the true price.
```{r}
model = xgb_best
pred = data.frame('Prediction'= predict(model,test_x),'True' = test_y)
ggplot(data=pred,aes(x=True,y=Prediction)) + geom_jitter() + geom_smooth(method='lm',size=.5) +
       theme_minimal(12) + labs(title=paste0('Prediction vs. Ground truth for ',model$method))
xgb_best_test_results = postResample(pred = pred$Prediction, obs = pred$True)
    
model = model_rpart
pred = data.frame('Prediction'= predict(model,test_x),'True' = test_y)
ggplot(data=pred,aes(x=True,y=Prediction)) + geom_jitter() + geom_smooth(method='lm',size=.5) +
       theme_minimal(12) + labs(title=paste0('Prediction vs. Ground truth for ',model$method))
rpart_test_results = postResample(pred = pred$Prediction, obs = pred$True)

model = model_glm
pred = data.frame('Prediction'= predict(model_glm,data_test_nodummy),'True' = data_test_nodummy$price)
ggplot(data=pred,aes(x=True,y=Prediction)) + geom_jitter() + geom_smooth(method='lm',size=.5) +
       theme_minimal(12) + labs(title=paste0('Prediction vs. Ground truth for ',model_glm$method))
glm_test_results = postResample(pred = pred$Prediction, obs = pred$True)

df_test_results = data.frame(xgb_best_test_results,rpart_test_results,glm_test_results)
df_test_results
```

Importance factors from the xgboost models.
```{r}
mat = xgb.importance (feature_names = colnames(train_x),model = xgb_best$finalModel)
xgb.plot.importance (importance_matrix = mat[1:10]) 
```
The models conclude that construction time is the most significant factors in determining the price (/m2). This is both intuitive and but not so intuitive. While it is definitely reasonable to say that construction time is an important factor, it's not immediately obvious to us that it is the most significant one. There are other factors such as number bathroom, building type, does not even make into the top10. 
## VII. Deployment

## VIII. Conclusions
